{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with Scikit-learn in Python\n",
    "**A tutorial by Thomas Jurczyk, CERES (Bochum)**\n",
    "\n",
    "This notebook includes the code used in my clustering tutorial for the [Programming Historian](https://programminghistorian.org/). Please note that you need to save the datasets `DNP_ancient_authors.csv` and `RELIGION_abstracts.csv` in the a folder called `data`. The datasets are available in my [GitHub repository](https://github.com/thomjur/clustering_with_sklearn_in_Python_PH).\n",
    "\n",
    "If you have any questions or comments, please contact me via [email](mailto:thomas.jurczyk-q88@rub.de).\n",
    "<br>\n",
    "<br>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNP Ancient Authors Data\n",
    "\n",
    "## 1. Loading the Datasets & Exploratory Data Analysis\n",
    "In the first step, we will load the `DNP_ancient_authors.csv` dataset into our program and look at some overview statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# load the authors dataset that has been stored as a .csv files in a folder called \"data\" in the same directory as the Jupyter Notebook\n",
    "df_authors = pd.read_csv(\"data/DNP_ancient_authors.csv\", index_col=\"authors\").drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we print out the first five rows and look at some information and overview statistcs about each dataset using pandas' `info()` and `describe()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see when using `describe()` on the `df_authors` dataset, we have an overall huge standard deviation in almost every column and a huge difference between the 75% percentil value and the maxim value. This indicates that we might have some serious outliers in our dataset, and it might make sense to get rid of them before we continue with our analysis. Therefore, we only keep those data points in our dataframe with a `word_count` within the 95% percentil range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ninety_quantile = df_authors[\"word_count\"].quantile(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors = df_authors[df_authors[\"word_count\"] <= ninety_quantile]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyzing the DNP Ancient Authors Dataset\n",
    "We start our analysis by clustering the `DNP_ancient_authors.csv` dataset. Before we start with the actual clustering process, we first import all the necessary library and write a couple of functions that will help us to plot our results during the analysis. We will also use these functions and imports in the second part of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler as SS # z-score normalization \n",
    "from sklearn.cluster import KMeans, DBSCAN # clustering algorithms\n",
    "from sklearn.decomposition import PCA # dimensionality reduction\n",
    "from sklearn.metrics import silhouette_score # used as a metric to evaluate the cohesion in a cluster\n",
    "from sklearn.neighbors import NearestNeighbors # for selecting the optimal eps value when using DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from yellowbrick.cluster import SilhouetteVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouettePlot(range_, data):\n",
    "    '''\n",
    "    we will use this function to plot a silhouette plot that helps us to evaluate the cohesion in clusters (k-means only)\n",
    "    '''\n",
    "    half_length = int(len(range_)/2)\n",
    "    range_list = list(range_)\n",
    "    fig, ax = plt.subplots(half_length, 2, figsize=(15,8))\n",
    "    for _ in range_:\n",
    "        kmeans = KMeans(n_clusters=_, random_state=42)\n",
    "        q, mod = divmod(_ - range_list[0], 2)\n",
    "        sv = SilhouetteVisualizer(kmeans, colors=\"yellowbrick\", ax=ax[q][mod])\n",
    "        ax[q][mod].set_title(\"Silhouette Plot with n={} Cluster\".format(_))\n",
    "        sv.fit(data)\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    fig.savefig(\"silhouette_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbowPlot(range_, data, figsize=(10,10)):\n",
    "    '''\n",
    "    the elbow plot function helps to figure out the right amount of clusters for a dataset\n",
    "    '''\n",
    "    inertia_list = []\n",
    "    for n in range_:\n",
    "        kmeans = KMeans(n_clusters=n, random_state=42)\n",
    "        kmeans.fit(data)\n",
    "        inertia_list.append(kmeans.inertia_)\n",
    "        \n",
    "    # plotting\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(111)\n",
    "    sns.lineplot(y=inertia_list, x=range_, ax=ax)\n",
    "    ax.set_xlabel(\"Cluster\")\n",
    "    ax.set_ylabel(\"Inertia\")\n",
    "    ax.set_xticks(list(range_))\n",
    "    fig.show()\n",
    "    fig.savefig(\"elbow_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findOptimalEps(n_neighbors, data):\n",
    "    '''\n",
    "    function to find optimal eps distance when using DBSCAN; based on this article: https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc\n",
    "    '''\n",
    "    neigh = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "    nbrs = neigh.fit(data)\n",
    "    distances, indices = nbrs.kneighbors(data)\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:,1]\n",
    "    plt.plot(distances)\n",
    "    plt.savefig(\"eps_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressiveFeatureSelection(df, n_clusters=3, max_features=4,):\n",
    "    '''\n",
    "    very basic implementation of an algorithm for feature selection (unsupervised clustering); taken from this post: https://datascience.stackexchange.com/questions/67040/how-to-do-feature-selection-for-clustering-and-implement-it-in-python\n",
    "    '''\n",
    "    feature_list = list(df.columns)\n",
    "    selected_features = list()\n",
    "    # select starting feature\n",
    "    initial_feature = \"\"\n",
    "    high_score = 0\n",
    "    for feature in feature_list:\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        data_ = df[feature]\n",
    "        labels = kmeans.fit_predict(data_.to_frame())\n",
    "        score_ = silhouette_score(data_.to_frame(), labels)\n",
    "        print(\"Proposed new feature {} with score {}\". format(feature, score_))\n",
    "        if score_ >= high_score:\n",
    "            initial_feature = feature\n",
    "            high_score = score_\n",
    "    print(\"The initial feature is {} with a silhouette score of {}.\".format(initial_feature, high_score))\n",
    "    feature_list.remove(initial_feature)\n",
    "    selected_features.append(initial_feature)\n",
    "    for _ in range(max_features-1):\n",
    "        high_score = 0\n",
    "        selected_feature = \"\"\n",
    "        print(\"Starting selection {}...\".format(_))\n",
    "        for feature in feature_list:\n",
    "            selection_ = selected_features.copy()\n",
    "            selection_.append(feature)\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "            data_ = df[selection_]\n",
    "            labels = kmeans.fit_predict(data_)\n",
    "            score_ = silhouette_score(data_, labels)\n",
    "            print(\"Proposed new feature {} with score {}\". format(feature, score_))\n",
    "            if score_ > high_score:\n",
    "                selected_feature = feature\n",
    "                high_score = score_\n",
    "        selected_features.append(selected_feature)\n",
    "        feature_list.remove(selected_feature)\n",
    "        print(\"Selected new feature {} with score {}\". format(selected_feature, high_score))\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalizing the DNP Ancient Authors Dataset\n",
    "Next, we initialize scikit-learn's `StandardScaler()` to normalize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = SS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNP_authors_normalized = scaler.fit_transform(df_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNP_authors_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors_normalized = pd.DataFrame(DNP_authors_normalized, columns=[\"word_count_normalized\", \"modern_translations_normalized\", \"known_works_normalized\", \"manuscripts_normalized\", \"early_editions_normalized\", \"early_translations_normalized\", \"modern_editions_normalized\", \"commentaries_normalized\"])\n",
    "df_authors_normalized = df_authors_normalized.set_index(df_authors.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection\n",
    "This next section is a little fuzzy. Selecting features in an unsupervised learning context is difficult. I have decided to implement a rather \"simple\" approach (see the function `progressiveFeatureSelection`). This attempt is based on [this algorithm presented on stackexchange](https://datascience.stackexchange.com/questions/67040/how-to-do-feature-selection-for-clustering-and-implement-it-in-python). Of course, there are more elaborate attempts to select features in an unsupervised learning context, such as presented in [this article](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.295.8115&rep=rep1&type=pdf). However, the method implemented here turned out to be quite successful in the context of the datasets used in this tutorial.\n",
    "\n",
    "For the sake of simplicity and since we only have ten features in the `DNP_ancient_authors.csv` dataset, we are looking for the ideal three features to cluster our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = progressiveFeatureSelection(df_authors_normalized, max_features=3, n_clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the features `known_works_normalized`, `commentaries_normalized`, and `modern_editions_normalized` might be worth using when trying to cluster our data. We are creating a new dataframe with only these three features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized_sliced = df_authors_normalized[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized_sliced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Figuring Out the Right Amount of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now apply the elbow method and then use silhouette plots to get an impression of how many clusters we should choose to analyse our dataset. We will check for two to ten clusters. Note, however, that the feature selection was also done with a pre-defined k-means algorithm using n=3 clusters. Thus, our three selected features might already have a tendency towards this amount of clusters, since they turned out to be the best choice under the parametric circumstances of n=3 cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbowPlot(range(1,11), df_normalized_sliced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the elbow plot indeed shows us that an \"elbow\" at n=3 as well as n=5 clusters. However, it's still pretty difficult to decide whether to use three, four, five or even six clusters. Therefore, we should also look at the silhouette plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouettePlot(range(3,9), df_normalized_sliced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the the silhouette scores underlines our previous intuition that the n=3 or n=5 seems to be the right choice of cluster. Particularly the silhouette plot with n=3 cluster has a relatively high average silhouette score. However, the different sizes of the \"knives\" as well as their sharp form indicate a single dominant cluster and a couple of rather small and less cohesive cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. n=3 K-Means Analysis of the DNP Ancient Authors Dataset\n",
    "Let us now train an k-means instance with n=3 clusters and plot the results using *seaborn*. Since we are focusing the plotting part in this tutorial on two dimensional plots, we wil us `PCA()` (Principal Component Analysis) to reduce the dimensionality to two dimensions for plotting reasons only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = kmeans.fit_predict(df_normalized_sliced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized_sliced[\"clusters\"] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using PCA to reduce the dimenionality to two dimenions (to be able to plot the data with *seaborn*)\n",
    "pca = PCA(n_components=2, whiten=False, random_state=42)\n",
    "authors_normalized_pca = pca.fit_transform(df_normalized_sliced)\n",
    "df_authors_normalized_pca = pd.DataFrame(data=authors_normalized_pca, columns=[\"pc_1\", \"pc_2\"])\n",
    "df_authors_normalized_pca[\"clusters\"] = cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors_normalized_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_plot = sns.scatterplot(x=\"pc_1\", y=\"pc_2\", hue=\"clusters\", data=df_authors_normalized_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns_plot.get_figure().savefig(\"dnp_ancient_authors_kmeans_plot_final_n=5.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks interesting! Even though the data is not perfectly clustered (which is something that we could already see during the evaluation of the silhouette plot), we can clearly separate at least one to two clusters. However, the two smaller ones still include quite a few outliers, which is a common problem of k-means.\n",
    "\n",
    "Let us now look at the entries in the three clusters and see if this actually delivered any valuable insights into our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors.iloc[df_authors_normalized_pca[df_authors_normalized_pca[\"clusters\"] == 0].index].describe() # authors with very few known works and few modern editions/commentaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors.iloc[df_authors_normalized_pca[df_authors_normalized_pca[\"clusters\"] == 1].index].describe() # authors many known works and many modern editions, but almost no commentaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors.iloc[df_authors_normalized_pca[df_authors_normalized_pca[\"clusters\"] == 1].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors.iloc[df_authors_normalized_pca[df_authors_normalized_pca[\"clusters\"] == 2].index].describe() # authors with few known works, but a lot of commentaries and relatively many modern editions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors.iloc[df_authors_normalized_pca[df_authors_normalized_pca[\"clusters\"] == 2].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors.iloc[df_authors_normalized_pca[df_authors_normalized_pca[\"clusters\"] == 3].index].describe() # authors with few commentaries and an average number of known works and modern editions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors.iloc[df_authors_normalized_pca[df_authors_normalized_pca[\"clusters\"] == 3].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors.iloc[df_authors_normalized_pca[df_authors_normalized_pca[\"clusters\"] == 4].index].describe() # authors with a very high amount of commentaries and an average amount of known works and modern editions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_authors.iloc[df_authors_normalized_pca[df_authors_normalized_pca[\"clusters\"] == 4].index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provided us with quite some interesting results. Note, however, that many of the known authors are missing due to our initial cut off of 10% of the data. In an actual research article, this might be a bad idea, however, it facilitated our clustering process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "# *Religion* Abstracts Data\n",
    "The second part of this tutorial is going to deal with textual data, namely all abstracts scraped from the [*Religion* (journal)](https://www.tandfonline.com/toc/rrel20/current) website. We will try to cluster the abstracts based on their word features in the form of **TF-IDF** vectors (which is short for \"**T**ext **F**requency - **I**nverted **D**ocument **F**requency\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Dataset & Exploratory Data Analysis\n",
    "Similar to the analysis of the `DNP_ancient_authors.csv` dataset, we will first load the `RELIGION_abstracts.csv` into our program and look at some descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstracts = pd.read_csv(\"data/RELIGION_abstracts_lemmatized.csv\").drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstracts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstracts.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF Vectorization\n",
    "\n",
    "In order to process the textual data with clustering algorithms, we need to convert the texts to vectors. For this purpose, we are using the scikit-learn implementation of **TF-IDF** vectorization. For a good introduction to how TF-IDF works, see this [great tutorial by Melanie Walsh](https://melaniewalsh.github.io/Intro-Cultural-Analytics/Text-Analysis/TF-IDF-Scikit-Learn.html).\n",
    "\n",
    "As an optional step, I have implemented a function called `lemmatizeAbstracts()` that lemmatizes the abstracts for us. Since we are not interested in stylistic similiarities between the abstracts, this might help us to reduce the overall amount of features (words) in our dataset. As part of the function, we are also cleaning the text of all punctuation and other noise such as brackets etc. In the following part, we are only working with the lemmatized version of the abstracts. However, if you feel like it, you can also continue using the original texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Optional Step*: Lemmatization of Abstracts (using *spacy*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# lemmatization (optional step)\n",
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatizeAbstracts(x):\n",
    "        doc = nlp(x)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            new_text.append(token.lemma_)\n",
    "        text_string = \" \".join(new_text)\n",
    "        # getting rid of non-word characters\n",
    "        text_string = re.sub(r\"[^\\w\\s]+\", \"\", text_string)\n",
    "        text_string = re.sub(r\"\\s{2,}\", \" \", text_string)\n",
    "        return text_string\n",
    "    \n",
    "df_abstracts[\"abstract_lemma\"] = df_abstracts[\"abstract\"].apply(lemmatizeAbstracts)\n",
    "df_abstracts.to_csv(\"data/RELIGION_abstracts_lemmatized.csv\")\n",
    "```\n",
    "\n",
    "I recommend saving the new lemmatized version of the dataset so that we do not have to redo the lemmatization each time we restart our notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to instantiate our TF-IDF model by passing it the argument to ignore stop words, such as \"the,\" \"a,\" etc. The second step is pretty similar to the training of our K-Means instance in the previous part: We are passing the abstracts from our dataset to the vectorizer in order to convert them to machine-readable vectors. For the moment, we are not passing any additional arguments. Finally, we create a new pandas DataFrame object based on the TF-IDF matrix of our textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstracts_tfidf = tfidf.fit_transform(df_abstracts[\"abstract_lemma\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstracts_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial matrix is *huge* and includes over 8,000 words from the overall vocabulary of the 701 abstracts. This is obviously too much, not only from a computational perspective, but also because clustering algorithms such as k-means loose much of their power due to the so-called *curse of dimensionality*. We will thus need to significantly reduce the number of features. To do so, we first create a new instance of the `df_abstracts_tfidf` with reduced maximum features to 250. We also tell the model to only consider words from the vocabulary that appear in at least five different documents but not more than 100 times. We also add the possibility to not only include single words, but bigrams as well (such as \"19th century\"). Finally, we tell our model to get rid of any potential accents.\n",
    "\n",
    "Secondly, we are also using the *Principal Component Analysis* (PCA), a technique that is often applied to reduce the dimensionality of datasets.\n",
    "\n",
    "> PCA allows us to reduce the dimensionality of the original data substantially while retaining most of the salient information. On the PCA-reduced feature set, other machine learning algorithms—downstream in the machine learning pipeline—will have an easier time separating the data points in space (to perform tasks such as anomaly detection and clustering) and will require fewer computational resources. (quote from the online version of Ankur A. Patel: *Hands-On Unsupervised Learning Using Python*, O'Reilly Media 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new TF-IDF matrix\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2), max_features=250, strip_accents=\"unicode\", min_df=10, max_df=200)\n",
    "tfidf_religion_array = tfidf.fit_transform(df_abstracts[\"abstract_lemma\"])\n",
    "df_abstracts_tfidf = pd.DataFrame(tfidf_religion_array.toarray(), index=df_abstracts.index, columns=tfidf.get_feature_names())\n",
    "df_abstracts_tfidf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dimensionality Reduction Using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the *curse of dimensionality* when using k-means, let us next use `PCA()` to caste the dimension from d=250 to d=10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using PCA to reduce the dimensionality\n",
    "pca = PCA(n_components=10, whiten=False, random_state=42)\n",
    "abstracts_pca = pca.fit_transform(df_abstracts_tfidf)\n",
    "df_abstracts_pca = pd.DataFrame(data=abstracts_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstracts_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Applying K-Means Clustering on Textual Data\n",
    "Next, we try to figure out if we can find any clusters in the abstracts using k-means. As we did in case of the `DNP_ancient_authors.csv` dataset, we will start by looking for the right amount of cluster using the elbow method and the silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbowPlot(range(10,100), df_abstracts_pca, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts_labels = kmeans.fit_predict(df_abstracts_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstracts_labeled = df_abstracts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstracts_labeled[\"cluster\"] = abstracts_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstracts_labeled[df_abstracts_labeled[\"cluster\"] == 84][\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works pretty well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Applying DBSCAN Clustering on Textual Data\n",
    "Even though the k-means clustering of our data already provided us with some valuable results from a clustering perspective, it might still be interesting to apply a different clustering algorithm such as DBSCAN. As explained above, DBSCAN allows for outliers in our data, meaning that it focuses on those regions in our data that may rightfully be called dense.\n",
    "\n",
    "We will be using the d=10 reduced version of our `RELIGION_abstracts.csv` dataset, which allows us to keep euclidean distance as a metric. If we were to use the initial TF-IDF matrix with 250 vectors, we should maybe change the underlying metric to cosine distance, which is better when dealing with sparse matrices such as in this example. \n",
    "\n",
    "The first step will be to figure out which eps value is most suitable for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findOptimalEps(2, df_abstracts_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 0.25 as eps value seems to be reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.2, min_samples=5, metric=\"euclidean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_labels = dbscan.fit_predict(df_abstracts_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstracts_dbscan = df_abstracts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstracts_dbscan[\"cluster\"] = dbscan_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstracts_dbscan[\"cluster\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_abstracts_dbscan[df_abstracts_dbscan[\"cluster\"] == 1][\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
